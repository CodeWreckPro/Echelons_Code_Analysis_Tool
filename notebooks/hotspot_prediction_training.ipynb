{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hotspot Prediction Model Training\n",
    "\n",
    "This notebook demonstrates the training process for our code hotspot prediction models. We'll use historical repository data to train models that can identify high-risk areas in the codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "# Custom imports\n",
    "from app.services.git_analysis import GitAnalysisService\n",
    "from app.services.embedding import EmbeddingService"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "First, we'll collect historical data from the Git repository to use as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def collect_training_data(repo_path):\n",
    "    git_service = GitAnalysisService()\n",
    "    git_service.initialize_repo(repo_path)\n",
    "    \n",
    "    data = []\n",
    "    for commit in git_service.repo.iter_commits():\n",
    "        for file in commit.stats.files:\n",
    "            stats = commit.stats.files[file]\n",
    "            data.append({\n",
    "                'file_path': file,\n",
    "                'lines_changed': stats['lines'],\n",
    "                'insertions': stats['insertions'],\n",
    "                'deletions': stats['deletions'],\n",
    "                'commit_frequency': 1,  # Will be aggregated later\n",
    "                'complexity': git_service._calculate_complexity(file),\n",
    "                'is_hotspot': False  # Will be determined based on metrics\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "We'll create relevant features for our model based on the collected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def engineer_features(df):\n",
    "    # Aggregate by file path\n",
    "    features = df.groupby('file_path').agg({\n",
    "        'lines_changed': 'sum',\n",
    "        'insertions': 'sum',\n",
    "        'deletions': 'sum',\n",
    "        'commit_frequency': 'sum',\n",
    "        'complexity': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Calculate additional features\n",
    "    features['change_rate'] = features['lines_changed'] / features['commit_frequency']\n",
    "    features['insertion_deletion_ratio'] = features['insertions'] / (features['deletions'] + 1)\n",
    "    \n",
    "    # Define hotspots (you may want to adjust these thresholds)\n",
    "    features['is_hotspot'] = (\n",
    "        (features['commit_frequency'] > features['commit_frequency'].quantile(0.75)) &\n",
    "        (features['complexity'] > features['complexity'].quantile(0.75))\n",
    "    )\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Now we'll train our Random Forest model to predict hotspots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_model(features):\n",
    "    # Prepare features and target\n",
    "    X = features[[\n",
    "        'lines_changed',\n",
    "        'commit_frequency',\n",
    "        'complexity',\n",
    "        'change_rate',\n",
    "        'insertion_deletion_ratio'\n",
    "    ]]\n",
    "    y = features['is_hotspot']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    train_score = model.score(X_train_scaled, y_train)\n",
    "    test_score = model.score(X_test_scaled, y_test)\n",
    "    \n",
    "    print(f\"Train accuracy: {train_score:.3f}\")\n",
    "    print(f\"Test accuracy: {test_score:.3f}\")\n",
    "    \n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model\n",
    "\n",
    "Finally, we'll save our trained model for use in the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def save_model(model, scaler, model_path, scaler_path):\n",
    "    joblib.dump(model, model_path)\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    print(f\"Scaler saved to {scaler_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training Pipeline\n",
    "\n",
    "Let's run our complete training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set paths\n",
    "REPO_PATH = \"path/to/your/repo\"\n",
    "MODEL_PATH = \"../ai/models/hotspot_prediction_model.joblib\"\n",
    "SCALER_PATH = \"../ai/models/hotspot_prediction_scaler.joblib\"\n",
    "\n",
    "# Collect and prepare data\n",
    "raw_data = collect_training_data(REPO_PATH)\n",
    "features = engineer_features(raw_data)\n",
    "\n",
    "# Train model\n",
    "model, scaler = train_model(features)\n",
    "\n",
    "# Save model and scaler\n",
    "save_model(model, scaler, MODEL_PATH, SCALER_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}